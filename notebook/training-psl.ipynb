{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "463dae00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-31T09:18:09.725909Z",
     "start_time": "2022-07-31T09:18:09.712003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/Desktop/Kaggle/Script/NLP/FPrize Effective Arguments/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a6c4be9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-31T09:18:12.241816Z",
     "start_time": "2022-07-31T09:18:10.288494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# from utils.viz import visualize\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM = true\n",
    "\n",
    "\n",
    "import warnings\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 500)\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52dec2e",
   "metadata": {},
   "source": [
    "test/ - A folder containing an example essay from the test set. The actual test set comprises about 3,000 essays in a format similar to the training set essays.\n",
    "The test set essays are distinct from the training set essays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a1ce74",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2284d8d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-31T09:18:12.245498Z",
     "start_time": "2022-07-31T09:18:12.243371Z"
    }
   },
   "outputs": [],
   "source": [
    "ROOT_DATA = Path('../data/')\n",
    "data_folder = ROOT_DATA/\"feedback-prize-effectiveness\"\n",
    "kfold_name = \"fold_k_5_seed_42\"\n",
    "TARGET = ['Ineffective','Adequate','Effective']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807dfc34",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bd28761",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-31T09:18:12.367952Z",
     "start_time": "2022-07-31T09:18:12.246739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144287, 110)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(data_folder/\"psl_train.csv\")\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0427efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"discourse_effectiveness\"]='Ineffective'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71c1c023",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-31T09:18:14.353420Z",
     "start_time": "2022-07-31T09:18:14.338502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_idx</th>\n",
       "      <th>dbl_ac_fold_0_Ineffective</th>\n",
       "      <th>dbl_ac_fold_0_Adequate</th>\n",
       "      <th>dbl_ac_fold_0_Effective</th>\n",
       "      <th>dbl_ac_fold_1_Ineffective</th>\n",
       "      <th>dbl_ac_fold_1_Adequate</th>\n",
       "      <th>dbl_ac_fold_1_Effective</th>\n",
       "      <th>dbl_ac_fold_2_Ineffective</th>\n",
       "      <th>dbl_ac_fold_2_Adequate</th>\n",
       "      <th>dbl_ac_fold_2_Effective</th>\n",
       "      <th>dbl_ac_fold_3_Ineffective</th>\n",
       "      <th>dbl_ac_fold_3_Adequate</th>\n",
       "      <th>dbl_ac_fold_3_Effective</th>\n",
       "      <th>dbl_ac_fold_4_Ineffective</th>\n",
       "      <th>dbl_ac_fold_4_Adequate</th>\n",
       "      <th>dbl_ac_fold_4_Effective</th>\n",
       "      <th>discourse_idx.1</th>\n",
       "      <th>dbl_ac_awp_fold_0_Ineffective</th>\n",
       "      <th>dbl_ac_awp_fold_0_Adequate</th>\n",
       "      <th>dbl_ac_awp_fold_0_Effective</th>\n",
       "      <th>dbl_ac_awp_fold_1_Ineffective</th>\n",
       "      <th>dbl_ac_awp_fold_1_Adequate</th>\n",
       "      <th>dbl_ac_awp_fold_1_Effective</th>\n",
       "      <th>dbl_ac_awp_fold_2_Ineffective</th>\n",
       "      <th>dbl_ac_awp_fold_2_Adequate</th>\n",
       "      <th>dbl_ac_awp_fold_2_Effective</th>\n",
       "      <th>dbl_ac_awp_fold_3_Ineffective</th>\n",
       "      <th>dbl_ac_awp_fold_3_Adequate</th>\n",
       "      <th>dbl_ac_awp_fold_3_Effective</th>\n",
       "      <th>dbl_ac_awp_fold_4_Ineffective</th>\n",
       "      <th>dbl_ac_awp_fold_4_Adequate</th>\n",
       "      <th>dbl_ac_awp_fold_4_Effective</th>\n",
       "      <th>discourse_idx.2</th>\n",
       "      <th>dbxl_ac_fold_0_Ineffective</th>\n",
       "      <th>dbxl_ac_fold_0_Adequate</th>\n",
       "      <th>dbxl_ac_fold_0_Effective</th>\n",
       "      <th>dbxl_ac_fold_1_Ineffective</th>\n",
       "      <th>dbxl_ac_fold_1_Adequate</th>\n",
       "      <th>dbxl_ac_fold_1_Effective</th>\n",
       "      <th>dbxl_ac_fold_2_Ineffective</th>\n",
       "      <th>dbxl_ac_fold_2_Adequate</th>\n",
       "      <th>dbxl_ac_fold_2_Effective</th>\n",
       "      <th>dbxl_ac_fold_3_Ineffective</th>\n",
       "      <th>dbxl_ac_fold_3_Adequate</th>\n",
       "      <th>dbxl_ac_fold_3_Effective</th>\n",
       "      <th>dbxl_ac_fold_4_Ineffective</th>\n",
       "      <th>dbxl_ac_fold_4_Adequate</th>\n",
       "      <th>dbxl_ac_fold_4_Effective</th>\n",
       "      <th>discourse_idx.3</th>\n",
       "      <th>dbl_cd_fold_0_Ineffective</th>\n",
       "      <th>dbl_cd_fold_0_Adequate</th>\n",
       "      <th>dbl_cd_fold_0_Effective</th>\n",
       "      <th>dbl_cd_fold_1_Ineffective</th>\n",
       "      <th>dbl_cd_fold_1_Adequate</th>\n",
       "      <th>dbl_cd_fold_1_Effective</th>\n",
       "      <th>dbl_cd_fold_2_Ineffective</th>\n",
       "      <th>dbl_cd_fold_2_Adequate</th>\n",
       "      <th>dbl_cd_fold_2_Effective</th>\n",
       "      <th>dbl_cd_fold_3_Ineffective</th>\n",
       "      <th>dbl_cd_fold_3_Adequate</th>\n",
       "      <th>dbl_cd_fold_3_Effective</th>\n",
       "      <th>dbl_cd_fold_4_Ineffective</th>\n",
       "      <th>dbl_cd_fold_4_Adequate</th>\n",
       "      <th>dbl_cd_fold_4_Effective</th>\n",
       "      <th>discourse_idx.4</th>\n",
       "      <th>dbxl_cd_fold_0_Ineffective</th>\n",
       "      <th>dbxl_cd_fold_0_Adequate</th>\n",
       "      <th>dbxl_cd_fold_0_Effective</th>\n",
       "      <th>dbxl_cd_fold_1_Ineffective</th>\n",
       "      <th>dbxl_cd_fold_1_Adequate</th>\n",
       "      <th>dbxl_cd_fold_1_Effective</th>\n",
       "      <th>dbxl_cd_fold_2_Ineffective</th>\n",
       "      <th>dbxl_cd_fold_2_Adequate</th>\n",
       "      <th>dbxl_cd_fold_2_Effective</th>\n",
       "      <th>dbxl_cd_fold_3_Ineffective</th>\n",
       "      <th>dbxl_cd_fold_3_Adequate</th>\n",
       "      <th>dbxl_cd_fold_3_Effective</th>\n",
       "      <th>dbxl_cd_fold_4_Ineffective</th>\n",
       "      <th>dbxl_cd_fold_4_Adequate</th>\n",
       "      <th>dbxl_cd_fold_4_Effective</th>\n",
       "      <th>discourse_idx.5</th>\n",
       "      <th>hf43_dr_fold_0_Ineffective</th>\n",
       "      <th>hf43_dr_fold_0_Adequate</th>\n",
       "      <th>hf43_dr_fold_0_Effective</th>\n",
       "      <th>hf43_dr_fold_1_Ineffective</th>\n",
       "      <th>hf43_dr_fold_1_Adequate</th>\n",
       "      <th>hf43_dr_fold_1_Effective</th>\n",
       "      <th>hf43_dr_fold_2_Ineffective</th>\n",
       "      <th>hf43_dr_fold_2_Adequate</th>\n",
       "      <th>hf43_dr_fold_2_Effective</th>\n",
       "      <th>hf43_dr_fold_3_Ineffective</th>\n",
       "      <th>hf43_dr_fold_3_Adequate</th>\n",
       "      <th>hf43_dr_fold_3_Effective</th>\n",
       "      <th>hf43_dr_fold_4_Ineffective</th>\n",
       "      <th>hf43_dr_fold_4_Adequate</th>\n",
       "      <th>hf43_dr_fold_4_Effective</th>\n",
       "      <th>fold_k_5_seed_42</th>\n",
       "      <th>fold_k_5_seed_2020</th>\n",
       "      <th>fold_k_8_seed_42</th>\n",
       "      <th>fold_k_8_seed_2020</th>\n",
       "      <th>fold_k_10_seed_42</th>\n",
       "      <th>fold_k_10_seed_2020</th>\n",
       "      <th>target</th>\n",
       "      <th>Ineffective</th>\n",
       "      <th>Adequate</th>\n",
       "      <th>Effective</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1622627660524</td>\n",
       "      <td>1078c641ef02</td>\n",
       "      <td>0.034966</td>\n",
       "      <td>0.954007</td>\n",
       "      <td>0.011027</td>\n",
       "      <td>0.018977</td>\n",
       "      <td>0.963046</td>\n",
       "      <td>0.017976</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>0.967579</td>\n",
       "      <td>0.017097</td>\n",
       "      <td>0.042056</td>\n",
       "      <td>0.941951</td>\n",
       "      <td>0.015993</td>\n",
       "      <td>0.038436</td>\n",
       "      <td>0.953973</td>\n",
       "      <td>0.007592</td>\n",
       "      <td>1078c641ef02</td>\n",
       "      <td>0.035761</td>\n",
       "      <td>0.953399</td>\n",
       "      <td>0.01084</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.895976</td>\n",
       "      <td>0.036227</td>\n",
       "      <td>0.085879</td>\n",
       "      <td>0.883173</td>\n",
       "      <td>0.030948</td>\n",
       "      <td>0.078403</td>\n",
       "      <td>0.907612</td>\n",
       "      <td>0.013985</td>\n",
       "      <td>0.062033</td>\n",
       "      <td>0.927665</td>\n",
       "      <td>0.010302</td>\n",
       "      <td>1078c641ef02</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>0.954974</td>\n",
       "      <td>0.012031</td>\n",
       "      <td>0.032231</td>\n",
       "      <td>0.952640</td>\n",
       "      <td>0.015129</td>\n",
       "      <td>0.042135</td>\n",
       "      <td>0.946509</td>\n",
       "      <td>0.011356</td>\n",
       "      <td>0.015863</td>\n",
       "      <td>0.955661</td>\n",
       "      <td>0.028476</td>\n",
       "      <td>0.019859</td>\n",
       "      <td>0.975816</td>\n",
       "      <td>0.004325</td>\n",
       "      <td>1078c641ef02</td>\n",
       "      <td>0.017759</td>\n",
       "      <td>0.961099</td>\n",
       "      <td>0.021142</td>\n",
       "      <td>0.078091</td>\n",
       "      <td>0.907290</td>\n",
       "      <td>0.014619</td>\n",
       "      <td>0.037290</td>\n",
       "      <td>0.911325</td>\n",
       "      <td>0.051385</td>\n",
       "      <td>0.038027</td>\n",
       "      <td>0.955099</td>\n",
       "      <td>0.006874</td>\n",
       "      <td>0.037068</td>\n",
       "      <td>0.954009</td>\n",
       "      <td>0.008923</td>\n",
       "      <td>1078c641ef02</td>\n",
       "      <td>0.024958</td>\n",
       "      <td>0.960981</td>\n",
       "      <td>0.014062</td>\n",
       "      <td>0.055443</td>\n",
       "      <td>0.936815</td>\n",
       "      <td>0.007742</td>\n",
       "      <td>0.048652</td>\n",
       "      <td>0.947200</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.026633</td>\n",
       "      <td>0.949759</td>\n",
       "      <td>0.023609</td>\n",
       "      <td>0.023523</td>\n",
       "      <td>0.967084</td>\n",
       "      <td>0.009393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026505</td>\n",
       "      <td>0.945362</td>\n",
       "      <td>0.028132</td>\n",
       "      <td>0.035739</td>\n",
       "      <td>0.945428</td>\n",
       "      <td>0.018833</td>\n",
       "      <td>0.031681</td>\n",
       "      <td>0.939529</td>\n",
       "      <td>0.028790</td>\n",
       "      <td>0.019637</td>\n",
       "      <td>0.962005</td>\n",
       "      <td>0.018357</td>\n",
       "      <td>0.024594</td>\n",
       "      <td>0.963408</td>\n",
       "      <td>0.011998</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.038277</td>\n",
       "      <td>0.944679</td>\n",
       "      <td>0.017044</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Ineffective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1622627653021</td>\n",
       "      <td>3695c85c6495</td>\n",
       "      <td>0.197380</td>\n",
       "      <td>0.798637</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>0.076309</td>\n",
       "      <td>0.918173</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>0.124825</td>\n",
       "      <td>0.872499</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>0.220555</td>\n",
       "      <td>0.776049</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>0.202561</td>\n",
       "      <td>0.795569</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>3695c85c6495</td>\n",
       "      <td>0.108020</td>\n",
       "      <td>0.889190</td>\n",
       "      <td>0.00279</td>\n",
       "      <td>0.114678</td>\n",
       "      <td>0.881405</td>\n",
       "      <td>0.003916</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.855806</td>\n",
       "      <td>0.005508</td>\n",
       "      <td>0.145438</td>\n",
       "      <td>0.850422</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.175941</td>\n",
       "      <td>0.820701</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>3695c85c6495</td>\n",
       "      <td>0.056605</td>\n",
       "      <td>0.939123</td>\n",
       "      <td>0.004272</td>\n",
       "      <td>0.077579</td>\n",
       "      <td>0.918072</td>\n",
       "      <td>0.004350</td>\n",
       "      <td>0.098522</td>\n",
       "      <td>0.899708</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.038785</td>\n",
       "      <td>0.956055</td>\n",
       "      <td>0.005160</td>\n",
       "      <td>0.104478</td>\n",
       "      <td>0.894190</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>3695c85c6495</td>\n",
       "      <td>0.040795</td>\n",
       "      <td>0.954139</td>\n",
       "      <td>0.005066</td>\n",
       "      <td>0.079454</td>\n",
       "      <td>0.916885</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.058666</td>\n",
       "      <td>0.927609</td>\n",
       "      <td>0.013724</td>\n",
       "      <td>0.072862</td>\n",
       "      <td>0.923556</td>\n",
       "      <td>0.003582</td>\n",
       "      <td>0.147645</td>\n",
       "      <td>0.849236</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>3695c85c6495</td>\n",
       "      <td>0.113238</td>\n",
       "      <td>0.879634</td>\n",
       "      <td>0.007129</td>\n",
       "      <td>0.124125</td>\n",
       "      <td>0.865935</td>\n",
       "      <td>0.009940</td>\n",
       "      <td>0.046513</td>\n",
       "      <td>0.949099</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>0.036960</td>\n",
       "      <td>0.951375</td>\n",
       "      <td>0.011665</td>\n",
       "      <td>0.128466</td>\n",
       "      <td>0.862962</td>\n",
       "      <td>0.008572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.109641</td>\n",
       "      <td>0.885871</td>\n",
       "      <td>0.004488</td>\n",
       "      <td>0.103145</td>\n",
       "      <td>0.892557</td>\n",
       "      <td>0.004298</td>\n",
       "      <td>0.078563</td>\n",
       "      <td>0.918519</td>\n",
       "      <td>0.002918</td>\n",
       "      <td>0.034002</td>\n",
       "      <td>0.959838</td>\n",
       "      <td>0.006159</td>\n",
       "      <td>0.091991</td>\n",
       "      <td>0.904461</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.104881</td>\n",
       "      <td>0.890243</td>\n",
       "      <td>0.004876</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Ineffective</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id   discourse_id discourse_idx  dbl_ac_fold_0_Ineffective  dbl_ac_fold_0_Adequate  dbl_ac_fold_0_Effective  dbl_ac_fold_1_Ineffective  dbl_ac_fold_1_Adequate  dbl_ac_fold_1_Effective  dbl_ac_fold_2_Ineffective  dbl_ac_fold_2_Adequate  dbl_ac_fold_2_Effective  dbl_ac_fold_3_Ineffective  dbl_ac_fold_3_Adequate  dbl_ac_fold_3_Effective  dbl_ac_fold_4_Ineffective  dbl_ac_fold_4_Adequate  dbl_ac_fold_4_Effective discourse_idx.1  dbl_ac_awp_fold_0_Ineffective  \\\n",
       "0  423A1CA112E2  1622627660524  1078c641ef02                   0.034966                0.954007                 0.011027                   0.018977                0.963046                 0.017976                   0.015324                0.967579                 0.017097                   0.042056                0.941951                 0.015993                   0.038436                0.953973                 0.007592    1078c641ef02                       0.035761   \n",
       "1  423A1CA112E2  1622627653021  3695c85c6495                   0.197380                0.798637                 0.003982                   0.076309                0.918173                 0.005518                   0.124825                0.872499                 0.002675                   0.220555                0.776049                 0.003396                   0.202561                0.795569                 0.001870    3695c85c6495                       0.108020   \n",
       "\n",
       "   dbl_ac_awp_fold_0_Adequate  dbl_ac_awp_fold_0_Effective  dbl_ac_awp_fold_1_Ineffective  dbl_ac_awp_fold_1_Adequate  dbl_ac_awp_fold_1_Effective  dbl_ac_awp_fold_2_Ineffective  dbl_ac_awp_fold_2_Adequate  dbl_ac_awp_fold_2_Effective  dbl_ac_awp_fold_3_Ineffective  dbl_ac_awp_fold_3_Adequate  dbl_ac_awp_fold_3_Effective  dbl_ac_awp_fold_4_Ineffective  dbl_ac_awp_fold_4_Adequate  dbl_ac_awp_fold_4_Effective discourse_idx.2  dbxl_ac_fold_0_Ineffective  dbxl_ac_fold_0_Adequate  \\\n",
       "0                    0.953399                      0.01084                       0.067797                    0.895976                     0.036227                       0.085879                    0.883173                     0.030948                       0.078403                    0.907612                     0.013985                       0.062033                    0.927665                     0.010302    1078c641ef02                    0.032995                 0.954974   \n",
       "1                    0.889190                      0.00279                       0.114678                    0.881405                     0.003916                       0.138686                    0.855806                     0.005508                       0.145438                    0.850422                     0.004140                       0.175941                    0.820701                     0.003357    3695c85c6495                    0.056605                 0.939123   \n",
       "\n",
       "   dbxl_ac_fold_0_Effective  dbxl_ac_fold_1_Ineffective  dbxl_ac_fold_1_Adequate  dbxl_ac_fold_1_Effective  dbxl_ac_fold_2_Ineffective  dbxl_ac_fold_2_Adequate  dbxl_ac_fold_2_Effective  dbxl_ac_fold_3_Ineffective  dbxl_ac_fold_3_Adequate  dbxl_ac_fold_3_Effective  dbxl_ac_fold_4_Ineffective  dbxl_ac_fold_4_Adequate  dbxl_ac_fold_4_Effective discourse_idx.3  dbl_cd_fold_0_Ineffective  dbl_cd_fold_0_Adequate  dbl_cd_fold_0_Effective  dbl_cd_fold_1_Ineffective  dbl_cd_fold_1_Adequate  \\\n",
       "0                  0.012031                    0.032231                 0.952640                  0.015129                    0.042135                 0.946509                  0.011356                    0.015863                 0.955661                  0.028476                    0.019859                 0.975816                  0.004325    1078c641ef02                   0.017759                0.961099                 0.021142                   0.078091                0.907290   \n",
       "1                  0.004272                    0.077579                 0.918072                  0.004350                    0.098522                 0.899708                  0.001770                    0.038785                 0.956055                  0.005160                    0.104478                 0.894190                  0.001332    3695c85c6495                   0.040795                0.954139                 0.005066                   0.079454                0.916885   \n",
       "\n",
       "   dbl_cd_fold_1_Effective  dbl_cd_fold_2_Ineffective  dbl_cd_fold_2_Adequate  dbl_cd_fold_2_Effective  dbl_cd_fold_3_Ineffective  dbl_cd_fold_3_Adequate  dbl_cd_fold_3_Effective  dbl_cd_fold_4_Ineffective  dbl_cd_fold_4_Adequate  dbl_cd_fold_4_Effective discourse_idx.4  dbxl_cd_fold_0_Ineffective  dbxl_cd_fold_0_Adequate  dbxl_cd_fold_0_Effective  dbxl_cd_fold_1_Ineffective  dbxl_cd_fold_1_Adequate  dbxl_cd_fold_1_Effective  dbxl_cd_fold_2_Ineffective  dbxl_cd_fold_2_Adequate  \\\n",
       "0                 0.014619                   0.037290                0.911325                 0.051385                   0.038027                0.955099                 0.006874                   0.037068                0.954009                 0.008923    1078c641ef02                    0.024958                 0.960981                  0.014062                    0.055443                 0.936815                  0.007742                    0.048652                 0.947200   \n",
       "1                 0.003661                   0.058666                0.927609                 0.013724                   0.072862                0.923556                 0.003582                   0.147645                0.849236                 0.003119    3695c85c6495                    0.113238                 0.879634                  0.007129                    0.124125                 0.865935                  0.009940                    0.046513                 0.949099   \n",
       "\n",
       "   dbxl_cd_fold_2_Effective  dbxl_cd_fold_3_Ineffective  dbxl_cd_fold_3_Adequate  dbxl_cd_fold_3_Effective  dbxl_cd_fold_4_Ineffective  dbxl_cd_fold_4_Adequate  dbxl_cd_fold_4_Effective  discourse_idx.5  hf43_dr_fold_0_Ineffective  hf43_dr_fold_0_Adequate  hf43_dr_fold_0_Effective  hf43_dr_fold_1_Ineffective  hf43_dr_fold_1_Adequate  hf43_dr_fold_1_Effective  hf43_dr_fold_2_Ineffective  hf43_dr_fold_2_Adequate  hf43_dr_fold_2_Effective  hf43_dr_fold_3_Ineffective  hf43_dr_fold_3_Adequate  \\\n",
       "0                  0.004149                    0.026633                 0.949759                  0.023609                    0.023523                 0.967084                  0.009393              NaN                    0.026505                 0.945362                  0.028132                    0.035739                 0.945428                  0.018833                    0.031681                 0.939529                  0.028790                    0.019637                 0.962005   \n",
       "1                  0.004388                    0.036960                 0.951375                  0.011665                    0.128466                 0.862962                  0.008572              NaN                    0.109641                 0.885871                  0.004488                    0.103145                 0.892557                  0.004298                    0.078563                 0.918519                  0.002918                    0.034002                 0.959838   \n",
       "\n",
       "   hf43_dr_fold_3_Effective  hf43_dr_fold_4_Ineffective  hf43_dr_fold_4_Adequate  hf43_dr_fold_4_Effective  fold_k_5_seed_42  fold_k_5_seed_2020  fold_k_8_seed_42  fold_k_8_seed_2020  fold_k_10_seed_42  fold_k_10_seed_2020  target  Ineffective  Adequate  Effective                                     discourse_text discourse_type discourse_effectiveness  \n",
       "0                  0.018357                    0.024594                 0.963408                  0.011998               4.0                 2.0               5.0                 4.0                8.0                  8.0       1     0.038277  0.944679   0.017044  Modern humans today are always on their phone....           Lead             Ineffective  \n",
       "1                  0.006159                    0.091991                 0.904461                  0.003548               4.0                 2.0               5.0                 4.0                8.0                  8.0       1     0.104881  0.890243   0.004876  They are some really bad consequences when stu...       Position             Ineffective  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4c5e783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    20972\n",
       "2     9326\n",
       "0     6461\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[train_df[kfold_name]!=10].target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08239e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dbl_ac_fold_0_Ineffective', 'dbl_ac_awp_fold_0_Ineffective', 'dbxl_ac_fold_0_Ineffective', 'dbl_cd_fold_0_Ineffective', 'dbxl_cd_fold_0_Ineffective', 'hf43_dr_fold_0_Ineffective']\n",
      "['dbl_ac_fold_0_Adequate', 'dbl_ac_awp_fold_0_Adequate', 'dbxl_ac_fold_0_Adequate', 'dbl_cd_fold_0_Adequate', 'dbxl_cd_fold_0_Adequate', 'hf43_dr_fold_0_Adequate']\n",
      "['dbl_ac_fold_0_Effective', 'dbl_ac_awp_fold_0_Effective', 'dbxl_ac_fold_0_Effective', 'dbl_cd_fold_0_Effective', 'dbxl_cd_fold_0_Effective', 'hf43_dr_fold_0_Effective']\n"
     ]
    }
   ],
   "source": [
    "for c in TARGET:\n",
    "    cols = [x for x in train_df.columns if x!=c  if c in x if 'fold_0' in x]\n",
    "    print(cols)\n",
    "    train_df[c] = train_df[cols].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62f14268",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fold0 = train_df[train_df[kfold_name]==0]\n",
    "df_fold0[kfold_name] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "022a00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_df[kfold_name]!=10,TARGET] = pd.get_dummies(train_df.loc[train_df[kfold_name]!=10].target).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af93afae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_idx</th>\n",
       "      <th>dbl_ac_fold_0_Ineffective</th>\n",
       "      <th>dbl_ac_fold_0_Adequate</th>\n",
       "      <th>dbl_ac_fold_0_Effective</th>\n",
       "      <th>dbl_ac_fold_1_Ineffective</th>\n",
       "      <th>dbl_ac_fold_1_Adequate</th>\n",
       "      <th>dbl_ac_fold_1_Effective</th>\n",
       "      <th>dbl_ac_fold_2_Ineffective</th>\n",
       "      <th>dbl_ac_fold_2_Adequate</th>\n",
       "      <th>dbl_ac_fold_2_Effective</th>\n",
       "      <th>dbl_ac_fold_3_Ineffective</th>\n",
       "      <th>dbl_ac_fold_3_Adequate</th>\n",
       "      <th>dbl_ac_fold_3_Effective</th>\n",
       "      <th>dbl_ac_fold_4_Ineffective</th>\n",
       "      <th>dbl_ac_fold_4_Adequate</th>\n",
       "      <th>dbl_ac_fold_4_Effective</th>\n",
       "      <th>discourse_idx.1</th>\n",
       "      <th>dbl_ac_awp_fold_0_Ineffective</th>\n",
       "      <th>dbl_ac_awp_fold_0_Adequate</th>\n",
       "      <th>dbl_ac_awp_fold_0_Effective</th>\n",
       "      <th>dbl_ac_awp_fold_1_Ineffective</th>\n",
       "      <th>dbl_ac_awp_fold_1_Adequate</th>\n",
       "      <th>dbl_ac_awp_fold_1_Effective</th>\n",
       "      <th>dbl_ac_awp_fold_2_Ineffective</th>\n",
       "      <th>dbl_ac_awp_fold_2_Adequate</th>\n",
       "      <th>dbl_ac_awp_fold_2_Effective</th>\n",
       "      <th>dbl_ac_awp_fold_3_Ineffective</th>\n",
       "      <th>dbl_ac_awp_fold_3_Adequate</th>\n",
       "      <th>dbl_ac_awp_fold_3_Effective</th>\n",
       "      <th>dbl_ac_awp_fold_4_Ineffective</th>\n",
       "      <th>dbl_ac_awp_fold_4_Adequate</th>\n",
       "      <th>dbl_ac_awp_fold_4_Effective</th>\n",
       "      <th>discourse_idx.2</th>\n",
       "      <th>dbxl_ac_fold_0_Ineffective</th>\n",
       "      <th>dbxl_ac_fold_0_Adequate</th>\n",
       "      <th>dbxl_ac_fold_0_Effective</th>\n",
       "      <th>dbxl_ac_fold_1_Ineffective</th>\n",
       "      <th>dbxl_ac_fold_1_Adequate</th>\n",
       "      <th>dbxl_ac_fold_1_Effective</th>\n",
       "      <th>dbxl_ac_fold_2_Ineffective</th>\n",
       "      <th>dbxl_ac_fold_2_Adequate</th>\n",
       "      <th>dbxl_ac_fold_2_Effective</th>\n",
       "      <th>dbxl_ac_fold_3_Ineffective</th>\n",
       "      <th>dbxl_ac_fold_3_Adequate</th>\n",
       "      <th>dbxl_ac_fold_3_Effective</th>\n",
       "      <th>dbxl_ac_fold_4_Ineffective</th>\n",
       "      <th>dbxl_ac_fold_4_Adequate</th>\n",
       "      <th>dbxl_ac_fold_4_Effective</th>\n",
       "      <th>discourse_idx.3</th>\n",
       "      <th>dbl_cd_fold_0_Ineffective</th>\n",
       "      <th>dbl_cd_fold_0_Adequate</th>\n",
       "      <th>dbl_cd_fold_0_Effective</th>\n",
       "      <th>dbl_cd_fold_1_Ineffective</th>\n",
       "      <th>dbl_cd_fold_1_Adequate</th>\n",
       "      <th>dbl_cd_fold_1_Effective</th>\n",
       "      <th>dbl_cd_fold_2_Ineffective</th>\n",
       "      <th>dbl_cd_fold_2_Adequate</th>\n",
       "      <th>dbl_cd_fold_2_Effective</th>\n",
       "      <th>dbl_cd_fold_3_Ineffective</th>\n",
       "      <th>dbl_cd_fold_3_Adequate</th>\n",
       "      <th>dbl_cd_fold_3_Effective</th>\n",
       "      <th>dbl_cd_fold_4_Ineffective</th>\n",
       "      <th>dbl_cd_fold_4_Adequate</th>\n",
       "      <th>dbl_cd_fold_4_Effective</th>\n",
       "      <th>discourse_idx.4</th>\n",
       "      <th>dbxl_cd_fold_0_Ineffective</th>\n",
       "      <th>dbxl_cd_fold_0_Adequate</th>\n",
       "      <th>dbxl_cd_fold_0_Effective</th>\n",
       "      <th>dbxl_cd_fold_1_Ineffective</th>\n",
       "      <th>dbxl_cd_fold_1_Adequate</th>\n",
       "      <th>dbxl_cd_fold_1_Effective</th>\n",
       "      <th>dbxl_cd_fold_2_Ineffective</th>\n",
       "      <th>dbxl_cd_fold_2_Adequate</th>\n",
       "      <th>dbxl_cd_fold_2_Effective</th>\n",
       "      <th>dbxl_cd_fold_3_Ineffective</th>\n",
       "      <th>dbxl_cd_fold_3_Adequate</th>\n",
       "      <th>dbxl_cd_fold_3_Effective</th>\n",
       "      <th>dbxl_cd_fold_4_Ineffective</th>\n",
       "      <th>dbxl_cd_fold_4_Adequate</th>\n",
       "      <th>dbxl_cd_fold_4_Effective</th>\n",
       "      <th>discourse_idx.5</th>\n",
       "      <th>hf43_dr_fold_0_Ineffective</th>\n",
       "      <th>hf43_dr_fold_0_Adequate</th>\n",
       "      <th>hf43_dr_fold_0_Effective</th>\n",
       "      <th>hf43_dr_fold_1_Ineffective</th>\n",
       "      <th>hf43_dr_fold_1_Adequate</th>\n",
       "      <th>hf43_dr_fold_1_Effective</th>\n",
       "      <th>hf43_dr_fold_2_Ineffective</th>\n",
       "      <th>hf43_dr_fold_2_Adequate</th>\n",
       "      <th>hf43_dr_fold_2_Effective</th>\n",
       "      <th>hf43_dr_fold_3_Ineffective</th>\n",
       "      <th>hf43_dr_fold_3_Adequate</th>\n",
       "      <th>hf43_dr_fold_3_Effective</th>\n",
       "      <th>hf43_dr_fold_4_Ineffective</th>\n",
       "      <th>hf43_dr_fold_4_Adequate</th>\n",
       "      <th>hf43_dr_fold_4_Effective</th>\n",
       "      <th>fold_k_5_seed_42</th>\n",
       "      <th>fold_k_5_seed_2020</th>\n",
       "      <th>fold_k_8_seed_42</th>\n",
       "      <th>fold_k_8_seed_2020</th>\n",
       "      <th>fold_k_10_seed_42</th>\n",
       "      <th>fold_k_10_seed_2020</th>\n",
       "      <th>target</th>\n",
       "      <th>Ineffective</th>\n",
       "      <th>Adequate</th>\n",
       "      <th>Effective</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1622627660524</td>\n",
       "      <td>1078c641ef02</td>\n",
       "      <td>0.034966</td>\n",
       "      <td>0.954007</td>\n",
       "      <td>0.011027</td>\n",
       "      <td>0.018977</td>\n",
       "      <td>0.963046</td>\n",
       "      <td>0.017976</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>0.967579</td>\n",
       "      <td>0.017097</td>\n",
       "      <td>0.042056</td>\n",
       "      <td>0.941951</td>\n",
       "      <td>0.015993</td>\n",
       "      <td>0.038436</td>\n",
       "      <td>0.953973</td>\n",
       "      <td>0.007592</td>\n",
       "      <td>1078c641ef02</td>\n",
       "      <td>0.035761</td>\n",
       "      <td>0.953399</td>\n",
       "      <td>0.01084</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.895976</td>\n",
       "      <td>0.036227</td>\n",
       "      <td>0.085879</td>\n",
       "      <td>0.883173</td>\n",
       "      <td>0.030948</td>\n",
       "      <td>0.078403</td>\n",
       "      <td>0.907612</td>\n",
       "      <td>0.013985</td>\n",
       "      <td>0.062033</td>\n",
       "      <td>0.927665</td>\n",
       "      <td>0.010302</td>\n",
       "      <td>1078c641ef02</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>0.954974</td>\n",
       "      <td>0.012031</td>\n",
       "      <td>0.032231</td>\n",
       "      <td>0.952640</td>\n",
       "      <td>0.015129</td>\n",
       "      <td>0.042135</td>\n",
       "      <td>0.946509</td>\n",
       "      <td>0.011356</td>\n",
       "      <td>0.015863</td>\n",
       "      <td>0.955661</td>\n",
       "      <td>0.028476</td>\n",
       "      <td>0.019859</td>\n",
       "      <td>0.975816</td>\n",
       "      <td>0.004325</td>\n",
       "      <td>1078c641ef02</td>\n",
       "      <td>0.017759</td>\n",
       "      <td>0.961099</td>\n",
       "      <td>0.021142</td>\n",
       "      <td>0.078091</td>\n",
       "      <td>0.907290</td>\n",
       "      <td>0.014619</td>\n",
       "      <td>0.037290</td>\n",
       "      <td>0.911325</td>\n",
       "      <td>0.051385</td>\n",
       "      <td>0.038027</td>\n",
       "      <td>0.955099</td>\n",
       "      <td>0.006874</td>\n",
       "      <td>0.037068</td>\n",
       "      <td>0.954009</td>\n",
       "      <td>0.008923</td>\n",
       "      <td>1078c641ef02</td>\n",
       "      <td>0.024958</td>\n",
       "      <td>0.960981</td>\n",
       "      <td>0.014062</td>\n",
       "      <td>0.055443</td>\n",
       "      <td>0.936815</td>\n",
       "      <td>0.007742</td>\n",
       "      <td>0.048652</td>\n",
       "      <td>0.947200</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.026633</td>\n",
       "      <td>0.949759</td>\n",
       "      <td>0.023609</td>\n",
       "      <td>0.023523</td>\n",
       "      <td>0.967084</td>\n",
       "      <td>0.009393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026505</td>\n",
       "      <td>0.945362</td>\n",
       "      <td>0.028132</td>\n",
       "      <td>0.035739</td>\n",
       "      <td>0.945428</td>\n",
       "      <td>0.018833</td>\n",
       "      <td>0.031681</td>\n",
       "      <td>0.939529</td>\n",
       "      <td>0.028790</td>\n",
       "      <td>0.019637</td>\n",
       "      <td>0.962005</td>\n",
       "      <td>0.018357</td>\n",
       "      <td>0.024594</td>\n",
       "      <td>0.963408</td>\n",
       "      <td>0.011998</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Ineffective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1622627653021</td>\n",
       "      <td>3695c85c6495</td>\n",
       "      <td>0.197380</td>\n",
       "      <td>0.798637</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>0.076309</td>\n",
       "      <td>0.918173</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>0.124825</td>\n",
       "      <td>0.872499</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>0.220555</td>\n",
       "      <td>0.776049</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>0.202561</td>\n",
       "      <td>0.795569</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>3695c85c6495</td>\n",
       "      <td>0.108020</td>\n",
       "      <td>0.889190</td>\n",
       "      <td>0.00279</td>\n",
       "      <td>0.114678</td>\n",
       "      <td>0.881405</td>\n",
       "      <td>0.003916</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.855806</td>\n",
       "      <td>0.005508</td>\n",
       "      <td>0.145438</td>\n",
       "      <td>0.850422</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.175941</td>\n",
       "      <td>0.820701</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>3695c85c6495</td>\n",
       "      <td>0.056605</td>\n",
       "      <td>0.939123</td>\n",
       "      <td>0.004272</td>\n",
       "      <td>0.077579</td>\n",
       "      <td>0.918072</td>\n",
       "      <td>0.004350</td>\n",
       "      <td>0.098522</td>\n",
       "      <td>0.899708</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.038785</td>\n",
       "      <td>0.956055</td>\n",
       "      <td>0.005160</td>\n",
       "      <td>0.104478</td>\n",
       "      <td>0.894190</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>3695c85c6495</td>\n",
       "      <td>0.040795</td>\n",
       "      <td>0.954139</td>\n",
       "      <td>0.005066</td>\n",
       "      <td>0.079454</td>\n",
       "      <td>0.916885</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.058666</td>\n",
       "      <td>0.927609</td>\n",
       "      <td>0.013724</td>\n",
       "      <td>0.072862</td>\n",
       "      <td>0.923556</td>\n",
       "      <td>0.003582</td>\n",
       "      <td>0.147645</td>\n",
       "      <td>0.849236</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>3695c85c6495</td>\n",
       "      <td>0.113238</td>\n",
       "      <td>0.879634</td>\n",
       "      <td>0.007129</td>\n",
       "      <td>0.124125</td>\n",
       "      <td>0.865935</td>\n",
       "      <td>0.009940</td>\n",
       "      <td>0.046513</td>\n",
       "      <td>0.949099</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>0.036960</td>\n",
       "      <td>0.951375</td>\n",
       "      <td>0.011665</td>\n",
       "      <td>0.128466</td>\n",
       "      <td>0.862962</td>\n",
       "      <td>0.008572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.109641</td>\n",
       "      <td>0.885871</td>\n",
       "      <td>0.004488</td>\n",
       "      <td>0.103145</td>\n",
       "      <td>0.892557</td>\n",
       "      <td>0.004298</td>\n",
       "      <td>0.078563</td>\n",
       "      <td>0.918519</td>\n",
       "      <td>0.002918</td>\n",
       "      <td>0.034002</td>\n",
       "      <td>0.959838</td>\n",
       "      <td>0.006159</td>\n",
       "      <td>0.091991</td>\n",
       "      <td>0.904461</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Ineffective</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id   discourse_id discourse_idx  dbl_ac_fold_0_Ineffective  dbl_ac_fold_0_Adequate  dbl_ac_fold_0_Effective  dbl_ac_fold_1_Ineffective  dbl_ac_fold_1_Adequate  dbl_ac_fold_1_Effective  dbl_ac_fold_2_Ineffective  dbl_ac_fold_2_Adequate  dbl_ac_fold_2_Effective  dbl_ac_fold_3_Ineffective  dbl_ac_fold_3_Adequate  dbl_ac_fold_3_Effective  dbl_ac_fold_4_Ineffective  dbl_ac_fold_4_Adequate  dbl_ac_fold_4_Effective discourse_idx.1  dbl_ac_awp_fold_0_Ineffective  \\\n",
       "0  423A1CA112E2  1622627660524  1078c641ef02                   0.034966                0.954007                 0.011027                   0.018977                0.963046                 0.017976                   0.015324                0.967579                 0.017097                   0.042056                0.941951                 0.015993                   0.038436                0.953973                 0.007592    1078c641ef02                       0.035761   \n",
       "1  423A1CA112E2  1622627653021  3695c85c6495                   0.197380                0.798637                 0.003982                   0.076309                0.918173                 0.005518                   0.124825                0.872499                 0.002675                   0.220555                0.776049                 0.003396                   0.202561                0.795569                 0.001870    3695c85c6495                       0.108020   \n",
       "\n",
       "   dbl_ac_awp_fold_0_Adequate  dbl_ac_awp_fold_0_Effective  dbl_ac_awp_fold_1_Ineffective  dbl_ac_awp_fold_1_Adequate  dbl_ac_awp_fold_1_Effective  dbl_ac_awp_fold_2_Ineffective  dbl_ac_awp_fold_2_Adequate  dbl_ac_awp_fold_2_Effective  dbl_ac_awp_fold_3_Ineffective  dbl_ac_awp_fold_3_Adequate  dbl_ac_awp_fold_3_Effective  dbl_ac_awp_fold_4_Ineffective  dbl_ac_awp_fold_4_Adequate  dbl_ac_awp_fold_4_Effective discourse_idx.2  dbxl_ac_fold_0_Ineffective  dbxl_ac_fold_0_Adequate  \\\n",
       "0                    0.953399                      0.01084                       0.067797                    0.895976                     0.036227                       0.085879                    0.883173                     0.030948                       0.078403                    0.907612                     0.013985                       0.062033                    0.927665                     0.010302    1078c641ef02                    0.032995                 0.954974   \n",
       "1                    0.889190                      0.00279                       0.114678                    0.881405                     0.003916                       0.138686                    0.855806                     0.005508                       0.145438                    0.850422                     0.004140                       0.175941                    0.820701                     0.003357    3695c85c6495                    0.056605                 0.939123   \n",
       "\n",
       "   dbxl_ac_fold_0_Effective  dbxl_ac_fold_1_Ineffective  dbxl_ac_fold_1_Adequate  dbxl_ac_fold_1_Effective  dbxl_ac_fold_2_Ineffective  dbxl_ac_fold_2_Adequate  dbxl_ac_fold_2_Effective  dbxl_ac_fold_3_Ineffective  dbxl_ac_fold_3_Adequate  dbxl_ac_fold_3_Effective  dbxl_ac_fold_4_Ineffective  dbxl_ac_fold_4_Adequate  dbxl_ac_fold_4_Effective discourse_idx.3  dbl_cd_fold_0_Ineffective  dbl_cd_fold_0_Adequate  dbl_cd_fold_0_Effective  dbl_cd_fold_1_Ineffective  dbl_cd_fold_1_Adequate  \\\n",
       "0                  0.012031                    0.032231                 0.952640                  0.015129                    0.042135                 0.946509                  0.011356                    0.015863                 0.955661                  0.028476                    0.019859                 0.975816                  0.004325    1078c641ef02                   0.017759                0.961099                 0.021142                   0.078091                0.907290   \n",
       "1                  0.004272                    0.077579                 0.918072                  0.004350                    0.098522                 0.899708                  0.001770                    0.038785                 0.956055                  0.005160                    0.104478                 0.894190                  0.001332    3695c85c6495                   0.040795                0.954139                 0.005066                   0.079454                0.916885   \n",
       "\n",
       "   dbl_cd_fold_1_Effective  dbl_cd_fold_2_Ineffective  dbl_cd_fold_2_Adequate  dbl_cd_fold_2_Effective  dbl_cd_fold_3_Ineffective  dbl_cd_fold_3_Adequate  dbl_cd_fold_3_Effective  dbl_cd_fold_4_Ineffective  dbl_cd_fold_4_Adequate  dbl_cd_fold_4_Effective discourse_idx.4  dbxl_cd_fold_0_Ineffective  dbxl_cd_fold_0_Adequate  dbxl_cd_fold_0_Effective  dbxl_cd_fold_1_Ineffective  dbxl_cd_fold_1_Adequate  dbxl_cd_fold_1_Effective  dbxl_cd_fold_2_Ineffective  dbxl_cd_fold_2_Adequate  \\\n",
       "0                 0.014619                   0.037290                0.911325                 0.051385                   0.038027                0.955099                 0.006874                   0.037068                0.954009                 0.008923    1078c641ef02                    0.024958                 0.960981                  0.014062                    0.055443                 0.936815                  0.007742                    0.048652                 0.947200   \n",
       "1                 0.003661                   0.058666                0.927609                 0.013724                   0.072862                0.923556                 0.003582                   0.147645                0.849236                 0.003119    3695c85c6495                    0.113238                 0.879634                  0.007129                    0.124125                 0.865935                  0.009940                    0.046513                 0.949099   \n",
       "\n",
       "   dbxl_cd_fold_2_Effective  dbxl_cd_fold_3_Ineffective  dbxl_cd_fold_3_Adequate  dbxl_cd_fold_3_Effective  dbxl_cd_fold_4_Ineffective  dbxl_cd_fold_4_Adequate  dbxl_cd_fold_4_Effective  discourse_idx.5  hf43_dr_fold_0_Ineffective  hf43_dr_fold_0_Adequate  hf43_dr_fold_0_Effective  hf43_dr_fold_1_Ineffective  hf43_dr_fold_1_Adequate  hf43_dr_fold_1_Effective  hf43_dr_fold_2_Ineffective  hf43_dr_fold_2_Adequate  hf43_dr_fold_2_Effective  hf43_dr_fold_3_Ineffective  hf43_dr_fold_3_Adequate  \\\n",
       "0                  0.004149                    0.026633                 0.949759                  0.023609                    0.023523                 0.967084                  0.009393              NaN                    0.026505                 0.945362                  0.028132                    0.035739                 0.945428                  0.018833                    0.031681                 0.939529                  0.028790                    0.019637                 0.962005   \n",
       "1                  0.004388                    0.036960                 0.951375                  0.011665                    0.128466                 0.862962                  0.008572              NaN                    0.109641                 0.885871                  0.004488                    0.103145                 0.892557                  0.004298                    0.078563                 0.918519                  0.002918                    0.034002                 0.959838   \n",
       "\n",
       "   hf43_dr_fold_3_Effective  hf43_dr_fold_4_Ineffective  hf43_dr_fold_4_Adequate  hf43_dr_fold_4_Effective  fold_k_5_seed_42  fold_k_5_seed_2020  fold_k_8_seed_42  fold_k_8_seed_2020  fold_k_10_seed_42  fold_k_10_seed_2020  target  Ineffective  Adequate  Effective                                     discourse_text discourse_type discourse_effectiveness  \n",
       "0                  0.018357                    0.024594                 0.963408                  0.011998               4.0                 2.0               5.0                 4.0                8.0                  8.0       1          0.0       1.0        0.0  Modern humans today are always on their phone....           Lead             Ineffective  \n",
       "1                  0.006159                    0.091991                 0.904461                  0.003548               4.0                 2.0               5.0                 4.0                8.0                  8.0       1          0.0       1.0        0.0  They are some really bad consequences when stu...       Position             Ineffective  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9252430e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ineffective     6461.0\n",
       "Adequate       20972.0\n",
       "Effective       9326.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[train_df[kfold_name]!=10,TARGET].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7845573b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    7381\n",
       "Name: fold_k_5_seed_42, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fold0[kfold_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55bb8f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0    107528\n",
       "4.0       7441\n",
       "0.0       7381\n",
       "1.0       7343\n",
       "2.0       7318\n",
       "3.0       7276\n",
       "Name: fold_k_5_seed_42, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[kfold_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34a042c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151668, 111)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([train_df,df_fold0],axis=0).reset_index(drop=True)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a888b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144287, 111)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10effb7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-31T09:18:16.773094Z",
     "start_time": "2022-07-31T09:18:16.686826Z"
    }
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    seed = 2022\n",
    "    \n",
    "    # Model\n",
    "    model_name = \"microsoft/deberta-large\" #\"funnel-transformer/large\" #\"allenai/longformer-large-4096\"\n",
    "    \n",
    "    # CV\n",
    "    kfold_name = \"fold_k_5_seed_42\"\n",
    "    selected_folds = [0]\n",
    "    \n",
    "    # Paths\n",
    "    data_folder = ROOT_DATA/\"feedback-prize-2021\"\n",
    "    name = \"deberta_large\"\n",
    "    checkpoints_path = Path(fr'../checkpoint/{kfold_name}/{name}/psl')\n",
    "    pretrained_path = Path(fr'../checkpoint/MLM/{name}/')\n",
    "    add_special_tokens = True\n",
    "    input_type = \"cls_end\"\n",
    "    sample = False\n",
    "    mask_pct = 0\n",
    "    use_dropout = False\n",
    "    use_gradient_checkpointing = False\n",
    "    use_awp = False\n",
    "    val_score_min = 1.\n",
    "    grad_clip = False\n",
    "    add_soft_labels_fold = True\n",
    "    # Names\n",
    "    checkpoints_name = 'log'   \n",
    "    \n",
    "    model = {\n",
    "            \"max_len\":2048,\n",
    "            \"max_len_eval\":2048,\n",
    "            'loss':\"nn.CrossEntropyLoss\",\n",
    "            'pretrained_config':Path(fr'../checkpoint/MLM/{name}/config.pth'),\n",
    "            \"pretrained_weights\":None,\n",
    "            \"pretrained_tokenizer\":Path(fr'../checkpoint/MLM/{name}/tokenizer'),\n",
    "            \"num_labels\":3,\n",
    "            \"model_name\":model_name\n",
    "            }\n",
    "    \n",
    "    optimizer = {\n",
    "            \"name\":\"optim.AdamW\",\n",
    "            'params':{\"lr\":4e-6,'eps':1e-6,\"betas\":[0.9, 0.999],\n",
    "                     \"weight_decay\": 0.01\n",
    "                     },            \n",
    "            }\n",
    "\n",
    "    scheduler = {\n",
    "            \"name\":\"poly\",\n",
    "            'params':{\n",
    "                      \"lr_end\":3e-7,\"power\":3,\"epochs\":4\n",
    "                     },\n",
    "            \"warmup\":0.1,            \n",
    "            }\n",
    "    \n",
    "    train_loader = {\n",
    "            \"batch_size\":2,\n",
    "            'drop_last':True,\n",
    "            \"num_workers\":15,\n",
    "            \"pin_memory\":False,\n",
    "            \"shuffle\":True,\n",
    "            }\n",
    "    \n",
    "    val_loader = {\n",
    "            \"batch_size\":1,\n",
    "            'drop_last':False,\n",
    "            \"num_workers\":15,\n",
    "            \"pin_memory\":False,\n",
    "            \"shuffle\":False\n",
    "            }\n",
    "    trainer = {\"use_amp\":True,'epochs':4}\n",
    "    callbacks = {'save':True,\"es\":True,\"patience\":3,\n",
    "                 'verbose_eval':1,\"epoch_pct_eval\":1/10,\"epoch_eval_dist\":\"uniforme\",\n",
    "                 \"metric_track\":\"val_loss\",\"mode\":\"min\",'top_k':3,\"softmax_before\":0\n",
    "                }\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     device = torch.device(\"cpu\")\n",
    "    \n",
    "args.checkpoints_path.mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b44bff0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-31T09:18:17.787929Z",
     "start_time": "2022-07-31T09:18:17.582814Z"
    }
   },
   "outputs": [],
   "source": [
    "from train_utils_awp_psl import kfold,FeedbackModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bddd7a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-31T10:09:53.140449Z",
     "start_time": "2022-07-31T09:18:18.377058Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50279\n",
      "----------- fold_k_5_seed_42 ---------\n",
      "\n",
      "-------------   Fold 1 / 6  -------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c74edb23ac74c9eb8f179f715419409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d609d2c39d1d4ee79e039f9b9fc9d49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (747 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d5be2bae8742aa8d6a23b5870adf3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/838 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f60bd2a9f6f4d159c716f05815686e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/838 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pretrained Weights\n",
      "../checkpoint/MLM/deberta_large/_epoch=7 _step=1 _fold=0 _val_loss=1.3127 _val_log_loss=1.3127 _train_loss=1.7996.pth\n",
      "reseizing in nn\n",
      "Using Pretrained Weights\n",
      "../checkpoint/MLM/deberta_large/_epoch=7 _step=1 _fold=0 _val_loss=1.3127 _val_log_loss=1.3127 _train_loss=1.7996.pth\n",
      "    -> 405180419 trainable parameters\n",
      "\n",
      "Using Amp\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5cabd41308420bae3a6a285d8a7ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [cls_lead] You Ġand Ġyour Ġfriends Ġand Ġfamily Ġcould Ġoften Ġhave Ġarguments Ġon Ġwhether Ġsomething Ġis Ġreal Ġor Ġnot , Ġsuch Ġas Ġin Ġmovies . ĠSome Ġmight Ġthink Ġone Ġthing Ġis Ġspecial Ġeffects , Ġothers Ġwill Ġthink Ġit Ġis Ġreal . ĠIn ĠUn mask ing Ġthe ĠFace Ġon ĠMars , Ġthe Ġauthor Ġexplains Ġcontroversy Ġof Ġthe Ġ\" face \" Ġfound Ġon ĠMars ' Ġsurface . ĠIt Ġbecame Ġan Ġicon Ġonce Ġit Ġwas Ġreleased Ġto Ġthe Ġpublic . ĠIt Ġwas Ġhighlighted Ġfor Ġdecades Ġand Ġthe Ġsociety Ġargued Ġover Ġit Ġbeing Ġan Ġalien Ġmaking , Ġor Ġjust Ġa Ġland Ġform Ġcalled Ġa Ġmes a . [end_lead] Ġ [cls_position] The Ġface Ġwas Ġjust Ġa Ġnatural Ġland form Ġand Ġit Ġshouldn 't Ġhave Ġbeen Ġthought Ġas Ġan Ġalien Ġform . [end_position] ĊĊ [cls_claim] The Ġmost Ġlogical Ġanswer Ġto Ġthis Ġwas Ġthat Ġit Ġwas Ġa Ġmes a , Ġand Ġthat Ġthe Ġshadows Ġon Ġthe Ġsurface Ġhappened Ġto Ġmake Ġit Ġlook Ġlike Ġa Ġhuman Ġface Ġof Ġan Ġe gypt ian Ġph araoh . [end_claim] Ġ [cls_evidence] This Ġstruck Ġcivilization Ġin Ġthe Ġlate Ġ70 s Ġand Ġcaused Ġa Ġlot Ġof Ġarguments . ĠThe Ġpeople Ġthat Ġthought Ġis Ġactually Ġwas Ġan Ġalien Ġstructure Ġwere Ġjust Ġpeople Ġof Ġthe Ġpress Ġwho Ġjust Ġwanted Ġattention . ĠA ĠMars ĠGlobal ĠSurvey or Ġ\" Ġsnapped Ġa Ġpicture Ġten Ġtimes Ġsharper Ġthan Ġthe Ġoriginal ĠViking Ġphotos . ĠThousands Ġof Ġanxious Ġweb Ġsurf ers Ġwere Ġwaiting Ġwhen Ġthe Ġimage Ġfirst Ġappeared Ġan Ġa ĠJ PL Ġweb Ġsite , Ġrevealing ... Ġa Ġnatural Ġland form . ĠThere Ġwas Ġno Ġalien Ġmonument Ġafter Ġall .\" ĠThis Ġquotes Ġdirectly Ġstates Ġand Ġgives Ġevidence Ġthat Ġit Ġis Ġjust Ġa Ġnatural Ġland form . [end_evidence] ĊĊ [cls_claim] There Ġisn 't Ġeven Ġany Ġtrue Ġor Ġlogical Ġevidence Ġthat Ġthe Ġface Ġon ĠMars Ġwas Ġan Ġ\" Ġalien Ġartifact .\" ĠIt Ġwas Ġjust Ġa Ġrumor Ġthat Ġwent Ġaround Ġto Ġkeep Ġthings Ġinteresting Ġand Ġintrigue Ġbig Ġmovie Ġcompanies . [end_claim] Ġ [cls_evidence] The Ġcameras Ġeven Ġtook Ġa Ġpicture Ġwith Ġa Ġten Ġtimes Ġbetter Ġresolution Ġthan Ġthe Ġ1976 Ġpicture Ġof Ġthe Ġface . ĠIn Ġaddition , Ġthe Ġnew Ġpicture Ġeven Ġhad Ġeach Ġpixel Ġin Ġthe Ġimage Ġspan Ġ1 . 56 Ġmeters Ġin Ġcomparison Ġto Ġthe Ġoriginal Ġ43 Ġmeters Ġper Ġpixel . ĠGar vin Ġtalks Ġthat Ġ\" ĠAs Ġa Ġrule Ġof Ġthumb Ġyou Ġcan Ġdiscern Ġthings Ġin Ġa Ġdigital Ġimage Ġ3 Ġtimes Ġbigger Ġthan Ġthe Ġpixel Ġsize ,\" Ġand Ġthat Ġ\" Ġif Ġthere Ġwere Ġobjects Ġin Ġthis Ġpicture Ġlike Ġairplanes Ġon Ġthe Ġground Ġor ĠEgyptian - style Ġpy ramids Ġor Ġeven Ġsmall Ġsh acks , Ġyou Ġcould Ġsee Ġwhat Ġthey Ġwere !\" ĠGar vin Ġdirectly Ġstates Ġhere Ġthat Ġno Ġmatter Ġwhat , Ġthe Ġformation Ġcouldn 't Ġhave Ġpossibly Ġbeen Ġan Ġalien Ġformation Ġor Ġeven Ġa Ġpyramid . ĠThis Ġis Ġeven Ġmore Ġproof Ġthat Ġit Ġis Ġridiculous Ġto Ġthink Ġthat Ġthe Ġformation Ġwas Ġan Ġ\" Ġalien Ġmaking Ġ.\" [end_evidence] ĊĊ [cls_counterclaim] Although Ġthe Ġface Ġwas Ġthought Ġto Ġbe Ġalien [end_counterclaim] , Ġ [cls_rebuttal] there Ġis Ġtoo Ġmuch Ġevidence Ġthat Ġdenies Ġit , Ġand Ġthat Ġit Ġisn 't . [end_rebuttal] Ġ [cls_evidence] Scientists Ġwould Ġwish Ġthat Ġit Ġwas Ġan Ġextra Ġterrestrial Ġsign , Ġbut Ġthe Ġodds Ġof Ġthat Ġare Ġtoo Ġscarce Ġto Ġeven Ġconsider . ĠThis Ġwas Ġa Ġvery Ġcontroversial Ġtopic , Ġas ĠI Ġhave Ġstated Ġbefore . ĠAll Ġof Ġthe Ġtheorists Ġout Ġthere Ġhad Ġmany Ġdifferent Ġand Ġunique Ġopinions Ġof Ġit . ĠThe Ġnarrator Ġsays Ġwhat Ġthe Ġimage Ġ\" Ġactually Ġshows Ġis Ġthe ĠMartian Ġequivalent Ġof Ġa Ġbut te Ġor Ġmes a Ġ- Ġland from s Ġcommon Ġaround Ġthe ĠAmerican ĠWest . Ġ\" ĠIt Ġrem e ind s Ġme Ġmost Ġof ĠMiddle ĠBut te Ġin Ġthe ĠSnake ĠRiver ĠPlain Ġof ĠIdaho ,\" Ġsays ĠGar vin .\" ĠThis Ġevidence Ġgives Ġeven Ġfurther Ġsupport Ġthat Ġit Ġisn 't Ġextra Ġterrestrial , Ġand Ġstates Ġthat Ġit Ġisn 't , Ġsh ich Ġexposes Ġthat Ġit Ġis Ġa Ġfl uke . [end_evidence] ĊĊ [cls_concluding] The Ġface Ġon ĠMars Ġwas Ġan Ġongoing Ġargument Ġfor Ġyears Ġto Ġcome Ġonce Ġit Ġwas Ġreleased Ġto Ġthe Ġpublic . ĠThere Ġwas Ġno Ġreason Ġto Ġdo Ġso . ĠThere Ġwas Ġalready Ġvalid Ġpoints Ġthat Ġgave Ġproof Ġof Ġit Ġbeing Ġjust Ġa Ġmes a . ĠThings Ġlike Ġthis Ġocc ure nce Ġhappen Ġall Ġthe Ġtime , Ġeven Ġin Ġyour Ġdaily Ġlife . ĠIf Ġthings Ġlike Ġthis Ġrun Ġacross Ġyour Ġmind , Ġthink Ġabout Ġit Ġthe Ġlogical Ġway Ġand Ġnot Ġa Ġvery , Ġvery Ġunlikely Ġpath Ġabout Ġit . ĠThere Ġmay Ġbe Ġsimilarities Ġbetween Ġthe Ġsides , Ġlike Ġin Ġthis Ġargument . ĠBut , Ġthere Ġwas Ġtoo Ġmuch Ġto Ġshow Ġthat Ġit Ġfavored Ġthe Ġmore Ġrealistic Ġside Ġof Ġit . ĠThe Ġpassage Ġhas Ġmultiple Ġocc ure nces Ġwhere Ġit Ġdirectly Ġstates Ġjust Ġthat . [end_concluding] [SEP]\n",
      "Epoch 1.1/10 lr=0.000004 t=340s   train_loss=0.0823   val_loss=0.6807  val_log_loss=0.6807 \n",
      "Epoch 1.2/10 lr=0.000004 t=654s   train_loss=0.1498   val_loss=0.6226  val_log_loss=0.6226 \n",
      "Epoch 1.3/10 lr=0.000004 t=967s   train_loss=0.2147   val_loss=0.6004  val_log_loss=0.6004 \n",
      "Epoch 1.4/10 lr=0.000004 t=1275s   train_loss=0.2785   val_loss=0.5951  val_log_loss=0.5951 \n",
      "Epoch 1.5/10 lr=0.000004 t=1591s   train_loss=0.3413   val_loss=0.5935  val_log_loss=0.5935 \n",
      "Epoch 1.6/10 lr=0.000003 t=1903s   train_loss=0.4036   val_loss=0.5876  val_log_loss=0.5876 \n",
      "Epoch 1.7/10 lr=0.000003 t=2227s   train_loss=0.4652   val_loss=0.5839  val_log_loss=0.5839 \n",
      "Epoch 1.8/10 lr=0.000003 t=2539s   train_loss=0.5276   val_loss=0.5785  val_log_loss=0.5785 \n",
      "Epoch 1.9/10 lr=0.000003 t=2856s   train_loss=0.5900   val_loss=0.5730  val_log_loss=0.5730 \n",
      "Epoch 1.10/10 lr=0.000003 t=3168s   train_loss=0.6520   val_loss=0.5758  val_log_loss=0.5758 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa40a967545437381220e155f4f169e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 422.00 MiB (GPU 0; 47.51 GiB total capacity; 36.37 GiB already allocated; 85.94 MiB free; 39.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mkfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Kaggle/Script/NLP/FPrize Effective Arguments/src/train_utils_awp.py:720\u001b[0m, in \u001b[0;36mkfold\u001b[0;34m(args, df)\u001b[0m\n\u001b[1;32m    717\u001b[0m     train_df \u001b[38;5;241m=\u001b[39m df[df[args\u001b[38;5;241m.\u001b[39mkfold_name]\u001b[38;5;241m!=\u001b[39mi]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;66;03m#.sample(100)\u001b[39;00m\n\u001b[1;32m    718\u001b[0m     valid_df \u001b[38;5;241m=\u001b[39m df[df[args\u001b[38;5;241m.\u001b[39mkfold_name]\u001b[38;5;241m==\u001b[39mi]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;66;03m#.sample(100)\u001b[39;00m\n\u001b[0;32m--> 720\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalid_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Kaggle/Script/NLP/FPrize Effective Arguments/src/train_utils_awp.py:641\u001b[0m, in \u001b[0;36mtrain_one_fold\u001b[0;34m(args, tokenizer, train_df, valid_df, fold)\u001b[0m\n\u001b[1;32m    638\u001b[0m n_parameters \u001b[38;5;241m=\u001b[39m count_parameters(model)\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_parameters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m trainable parameters\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 641\u001b[0m pred_val \u001b[38;5;241m=\u001b[39m \u001b[43mfit_net\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pred_val\n",
      "File \u001b[0;32m~/Desktop/Kaggle/Script/NLP/FPrize Effective Arguments/src/train_utils_awp.py:501\u001b[0m, in \u001b[0;36mfit_net\u001b[0;34m(model, train_dataset, val_dataset, args, fold, tokenizer)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step\u001b[38;5;241m==\u001b[39mepoch \u001b[38;5;129;01mand\u001b[39;00m step\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(train_dataset\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])))\n\u001b[0;32m--> 501\u001b[0m loss,tr_sc\u001b[38;5;241m=\u001b[39m \u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix(tr_sc)\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m tr_sc\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/Desktop/Kaggle/Script/NLP/FPrize Effective Arguments/src/train_utils_awp.py:241\u001b[0m, in \u001b[0;36mtraining_step\u001b[0;34m(args, model, criterion, data)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mtrainer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_amp\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m--> 241\u001b[0m         pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Kaggle/Script/NLP/FPrize Effective Arguments/src/model_zoo/models.py:84\u001b[0m, in \u001b[0;36mFeedbackModel.forward\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,b):\n\u001b[0;32m---> 84\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     85\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     86\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py:954\u001b[0m, in \u001b[0;36mDebertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    944\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    946\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    947\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    948\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    951\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    952\u001b[0m )\n\u001b[0;32m--> 954\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    961\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py:447\u001b[0m, in \u001b[0;36mDebertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    438\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    439\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    440\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m         rel_embeddings,\n\u001b[1;32m    445\u001b[0m     )\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 447\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    457\u001b[0m     hidden_states, att_m \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py:352\u001b[0m, in \u001b[0;36mDebertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    351\u001b[0m ):\n\u001b[0;32m--> 352\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    361\u001b[0m         attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py:285\u001b[0m, in \u001b[0;36mDebertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    278\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    284\u001b[0m ):\n\u001b[0;32m--> 285\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    294\u001b[0m         self_output, att_matrix \u001b[38;5;241m=\u001b[39m self_output\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py:648\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    645\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_logits_proj(attention_scores\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    647\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m XSoftmax\u001b[38;5;241m.\u001b[39mapply(attention_scores, attention_mask, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 648\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtalking_head:\n\u001b[1;32m    650\u001b[0m     attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_weights_proj(attention_probs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py:208\u001b[0m, in \u001b[0;36mStableDropout.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03mCall the module\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    x (`torch.tensor`): The input tensor to apply dropout\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_prob \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mXDropout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py:169\u001b[0m, in \u001b[0;36mXDropout.forward\u001b[0;34m(ctx, input, local_ctx)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, \u001b[38;5;28minput\u001b[39m, local_ctx):\n\u001b[0;32m--> 169\u001b[0m     mask, dropout \u001b[38;5;241m=\u001b[39m \u001b[43mget_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dropout)\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dropout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py:155\u001b[0m, in \u001b[0;36mget_mask\u001b[0;34m(input, local_context)\u001b[0m\n\u001b[1;32m    152\u001b[0m     mask \u001b[38;5;241m=\u001b[39m local_context\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;28;01mif\u001b[39;00m local_context\u001b[38;5;241m.\u001b[39mreuse_mask \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_like\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbernoulli_(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dropout))\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(local_context, DropoutContext):\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m local_context\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 422.00 MiB (GPU 0; 47.51 GiB total capacity; 36.37 GiB already allocated; 85.94 MiB free; 39.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "kfold(args,train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fe8963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
